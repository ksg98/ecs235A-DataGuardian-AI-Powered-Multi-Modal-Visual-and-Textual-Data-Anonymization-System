{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Code Explanation\n",
        "\n",
        "#### Cell 12 - **Model Completion Operations**\n",
        "This cell defines the `ModelCompletionOperation` class, which is responsible for performing text and image-based model completion tasks using OpenAI APIs.\n",
        "\n",
        "**Key Features**:\n",
        "1. **Initialization (`__init__`)**:\n",
        "   - Reads the OpenAI API key from the project configuration and initializes the OpenAI client.\n",
        "   - Sets default parameters like `max_tokens` and `temperature` for API calls.\n",
        "\n",
        "2. **Methods**:\n",
        "   - `GetModel()`: Returns the initialized OpenAI client.\n",
        "   - `RunVision(initModel, targetPrompt, targetImage)`:\n",
        "     - Converts the input image to a base64-encoded string for API compatibility.\n",
        "     - Sends a multi-modal prompt (image and text) to the OpenAI GPT-4 model.\n",
        "     - Handles the response and extracts generated text, or returns an error if the API call fails.\n",
        "\n",
        "#### Cell 13 - **Recognizer Registry and Main Processing**\n",
        "This cell implements classes to define custom recognizers and register them for text analysis and anonymization using Presidio.\n",
        "\n",
        "**Key Classes**:\n",
        "1. **`GetRecognizers`**:\n",
        "   - Instantiates and configures recognizers for:\n",
        "     - Numbers (`NUMBER`)\n",
        "     - Credit cards (`CREDIT_CARD`)\n",
        "     - Emails (`EMAIL_ADDRESS`)\n",
        "     - URLs (`URL`)\n",
        "     - Phone numbers (`PHONE_NUMBER`)\n",
        "   - Supports multiple languages, including English (`en`), Spanish (`es`), French (`fr`), and more.\n",
        "\n",
        "2. **`GetRegistry`**:\n",
        "   - Manages a `RecognizerRegistry` instance to register all custom recognizers, including:\n",
        "     - Predefined recognizers such as `PERSONNAMERECOGNIZER`.\n",
        "     - Custom recognizers like `NUMBERRECOGNIZER`, `CREDITCARDRECOGNIZER`, etc.\n",
        "\n",
        "**Steps in Main Processing**:\n",
        "- Add each custom recognizer to the registry, enabling comprehensive text analysis for identifying sensitive entities.\n",
        "\n",
        "### Usage\n",
        "- The `ModelCompletionOperation` class can handle text and image completion tasks using GPT-4.\n",
        "- The `GetRecognizers` and `GetRegistry` classes integrate with Presidio to detect and anonymize sensitive entities from text, tailored to specific patterns (e.g., numbers, emails, URLs).\n"
      ],
      "metadata": {
        "id": "Ht8YJwpmNMe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Cell 12 - Model Completion Operations\n",
        "from openai import OpenAI\n",
        "\n",
        "class ModelCompletionOperation(object):\n",
        "    def __init__(self)->CLASSINIT:\n",
        "        self.openaiapikey = project_config[\"apikey\"][\"OPENAI_API_KEY\"]\n",
        "        self.client = OpenAI(api_key=self.openaiapikey)\n",
        "        self.defaultMaxToken = 100\n",
        "        self.temperature = 0.1\n",
        "\n",
        "    def GetModel(self)->MODEL:\n",
        "        return self.client\n",
        "\n",
        "    def RunVision(self, initModel:MODEL, targetPrompt:str, targetImage:IMAGE)->RESULT:\n",
        "        try:\n",
        "            # Convert image to base64\n",
        "            _, buffer = cv2.imencode(\".jpg\", targetImage)\n",
        "            encoded_frame = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "            # Create API call\n",
        "            response = initModel.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{encoded_frame}\"\n",
        "                            }\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": targetPrompt\n",
        "                        }\n",
        "                    ]\n",
        "                }],\n",
        "                max_tokens=self.defaultMaxToken,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "            # Extract response text\n",
        "            if response.choices:\n",
        "                return response.choices[0].message.content\n",
        "            else:\n",
        "                return \"No response generated\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"OpenAI API Error: {str(e)}\")\n",
        "            return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "8KUiW8sKcP50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 - Main Process Selection\n",
        "from presidio_analyzer.recognizer_registry import RecognizerRegistry\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
        "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
        "\n",
        "CONTEXTAWARE = LemmaContextAwareEnhancer(context_similarity_factor=0.2,\n",
        "                                        min_score_with_context_similarity=0.2)\n",
        "\n",
        "class GetRecognizers(object):\n",
        "    def __init__(self)->CLASSINIT:\n",
        "        self.supportedLanguage = [\"en\", \"es\", \"fr\", \"de\", \"ru\", \"nl\", \"xx\"]\n",
        "        self.numberRecognizer = NUMBERRECOGNIZER(supported_entities=[\"NUMBER\"],\n",
        "                                               supported_language=self.supportedLanguage)\n",
        "        self.creditcardRecognizer = CREDITCARDRECOGNIZER(supported_entities=[\"CREDIT_CARD\"],\n",
        "                                                       supported_language=self.supportedLanguage)\n",
        "        self.emailRecognizer = EMAILRECOGNIZER(supported_entities=[\"EMAIL_ADDRESS\"],\n",
        "                                             supported_language=self.supportedLanguage)\n",
        "        self.urlRecognizer = URLRECOGNIZER(supported_entities=[\"URL\"],\n",
        "                                         supported_language=self.supportedLanguage)\n",
        "        self.phoneRecognizer = PHONENUMBERRECOGNIZER(supported_entities=[\"PHONE_NUMBER\"],\n",
        "                                                   supported_language=self.supportedLanguage)\n",
        "\n",
        "class GetRegistry(object):\n",
        "    def __init__(self)->CLASSINIT:\n",
        "        self.recognizers = GetRecognizers()\n",
        "        self.registry = RecognizerRegistry()\n",
        "\n",
        "    def Run(self)->PROCESS:\n",
        "        self.registry.add_recognizer(recognizer=self.recognizers.numberRecognizer)\n",
        "        self.registry.add_recognizer(recognizer=self.recognizers.creditcardRecognizer)\n",
        "        self.registry.add_recognizer(recognizer=self.recognizers.emailRecognizer)\n",
        "        self.registry.add_recognizer(recognizer=self.recognizers.urlRecognizer)\n",
        "        self.registry.add_recognizer(recognizer=self.recognizers.phoneRecognizer)\n",
        "        self.registry.add_recognizer(recognizer=PERSONNAMERECOGNIZER)\n"
      ],
      "metadata": {
        "id": "7dk9wdq4cbWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's how to use the system:\n",
        "'''\n",
        "# Initialize the model operation\n",
        "model_op = ModelCompletionOperation()\n",
        "model = model_op.GetModel()\n",
        "\n",
        "# Example with an image (you'll need to load an image first)\n",
        "# image = cv.imread('your_image.jpg')\n",
        "# result = model_op.RunVision(model, \"What's in this image?\", image)\n",
        "# print(result)\n",
        "\n",
        "# Initialize recognizers and registry\n",
        "registry = GetRegistry()\n",
        "registry.Run()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "gnNlswiBcfXp",
        "outputId": "3ffd5efc-e96d-4d82-9610-fac6301de127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Initialize the model operation\\nmodel_op = ModelCompletionOperation()\\nmodel = model_op.GetModel()\\n\\n# Example with an image (you\\'ll need to load an image first)\\n# image = cv.imread(\\'your_image.jpg\\')\\n# result = model_op.RunVision(model, \"What\\'s in this image?\", image)\\n# print(result)\\n\\n# Initialize recognizers and registry\\nregistry = GetRegistry()\\nregistry.Run()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}